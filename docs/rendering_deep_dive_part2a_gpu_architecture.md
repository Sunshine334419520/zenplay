# 渲染深度解析 Part 2A：GPU 架构与并行计算基础

**文档目标**：深入理解 GPU 为什么比 CPU 快、GPU 的硬件架构、并行计算原理  
**前置知识**：Part 1（纹理基础）  
**阅读时间**：25-35 分钟

---

## 📚 目录

1. [CPU vs GPU：设计哲学的差异](#1-cpu-vs-gpu设计哲学的差异)
2. [GPU 硬件架构详解](#2-gpu-硬件架构详解)
3. [并行计算原理](#3-并行计算原理)
4. [GPU 内存系统](#4-gpu-内存系统)
5. [为什么渲染适合 GPU](#5-为什么渲染适合-gpu)

---

## 1. CPU vs GPU：设计哲学的差异

### 1.1 最直观的类比

```
CPU = 博士教授
- 少数几个人（4-16 个核心）
- 每个人都很聪明（复杂逻辑、分支预测）
- 擅长复杂问题（if-else、循环、递归）
- 顺序执行，前后依赖

GPU = 小学生工厂
- 成百上千人（数千个核心）
- 每个人只会简单任务（简单算术）
- 擅长大量重复劳动（每个像素做相同计算）
- 并行执行，互不依赖

例子：计算 100 万个数的平方

CPU 方法（4 个教授）：
教授 1：计算 0-249,999
教授 2：计算 250,000-499,999
教授 3：计算 500,000-749,999
教授 4：计算 750,000-999,999
时间：250,000 次计算 / 教授速度

GPU 方法（1000 个小学生）：
每个学生算 1000 个数
1000 人同时开始
时间：1000 次计算 / 学生速度

结果：即使学生慢，但人多力量大！
```

### 1.2 CPU 的设计哲学

```
┌────────────────────────────────────┐
│         CPU Core (核心)            │
├────────────────────────────────────┤
│  ┌──────────────────────────────┐  │
│  │   控制单元 (Control Unit)    │  │  ← 复杂逻辑
│  │  - 分支预测                  │  │
│  │  - 乱序执行                  │  │
│  │  - 指令调度                  │  │
│  └──────────────────────────────┘  │
│  ┌──────────────────────────────┐  │
│  │   大缓存 (Cache)             │  │  ← 每个核心独享
│  │  - L1: 64 KB                 │  │
│  │  - L2: 512 KB                │  │
│  │  - L3: 8-32 MB (共享)        │  │
│  └──────────────────────────────┘  │
│  ┌──────────────────────────────┐  │
│  │   算术逻辑单元 (ALU)         │  │  ← 功能强大
│  │  - 整数运算                  │  │
│  │  - 浮点运算                  │  │
│  │  - 向量运算 (AVX-512)        │  │
│  └──────────────────────────────┘  │
└────────────────────────────────────┘

特点：
✅ 单线程性能强（高频率：4-5 GHz）
✅ 缓存大（减少内存访问）
✅ 分支预测准（复杂控制流）
✅ 延迟低（每条指令快）
❌ 核心数少（4-16 个）
❌ 吞吐量低（总计算能力有限）

适用场景：
- 操作系统
- 数据库
- 编译器
- 复杂算法（排序、搜索）
- 任何有复杂 if-else 的代码
```

### 1.3 GPU 的设计哲学

```
┌────────────────────────────────────────────────────────┐
│                GPU (显卡)                               │
├────────────────────────────────────────────────────────┤
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐  │
│  │ SM 0     │ │ SM 1     │ │ SM 2     │ │ SM 63    │  │  ← 多个 SM
│  │ (流多处理器)│ │         │ │         │ │ ...      │  │     (RTX 4090: 128 个)
│  ├──────────┤ ├──────────┤ ├──────────┤ ├──────────┤  │
│  │64个核心  │ │64个核心  │ │64个核心  │ │64个核心  │  │  ← 每个 SM 有 64-128 核心
│  │+ 纹理单元│ │+ 纹理单元│ │+ 纹理单元│ │+ 纹理单元│  │
│  │+ 共享内存│ │+ 共享内存│ │+ 共享内存│ │+ 共享内存│  │
│  └──────────┘ └──────────┘ └──────────┘ └──────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │         显存 (VRAM)                              │  │  ← 大容量高带宽
│  │         12-24 GB, 带宽 ~1 TB/s                   │  │
│  └─────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────┘

特点：
✅ 核心数多（数千个）
✅ 吞吐量高（总计算能力强）
✅ 并行处理（同时处理数万像素）
✅ 内存带宽大（1 TB/s vs CPU 50 GB/s）
❌ 单线程慢（低频率：1-2 GHz）
❌ 缓存小（每个核心共享）
❌ 分支性能差（所有核心执行相同指令）
❌ 延迟高（单条指令慢，但靠并行弥补）

适用场景：
- 图形渲染
- 视频编解码
- 机器学习
- 科学计算
- 任何可以并行的大量简单任务
```

### 1.4 性能对比：实际数字

#### 理论性能（FLOPS = 浮点运算/秒）

```
Intel Core i9-13900K (2023)：
- 核心数：24 (8P + 16E)
- 频率：最高 5.8 GHz
- 理论性能：~2 TFLOPS (FP32)

NVIDIA RTX 4090 (2022)：
- 核心数：16,384 CUDA 核心
- 频率：2.5 GHz
- 理论性能：82.6 TFLOPS (FP32)

比率：GPU 是 CPU 的 41 倍！
```

#### 内存带宽

```
DDR5-6400 (CPU)：
- 双通道：2 × 32 位 = 64 位
- 频率：6400 MT/s
- 带宽：51.2 GB/s

GDDR6X (RTX 4090)：
- 384 位总线
- 频率：21 Gbps
- 带宽：1008 GB/s

比率：GPU 是 CPU 的 20 倍！
```

#### 实际渲染性能

```
任务：渲染 1920x1080 图像，每像素 10 次计算

数据：
- 像素数：2,073,600
- 总计算：20,736,000 次

CPU (16 线程)：
- 每线程处理：1,296,000 像素
- 单线程速度：1 GHz
- 时间：1,296,000 / 1,000,000,000 ≈ 1.3 ms
- 但有缓存未命中、分支等开销
- 实际时间：~30 ms (33 fps)

GPU (10,000 线程)：
- 每线程处理：207 像素
- 并行执行，所有线程同时开始
- 时间：207 / 线程速度 + 启动开销
- 实际时间：~1-2 ms (500-1000 fps)

比率：GPU 快 15-30 倍！
```

### 1.5 为什么不所有任务都用 GPU？

```cpp
// 场景 1：适合 GPU 的任务（大量独立计算）
void RenderPixels(Pixel* pixels, int count) {
    // GPU 并行执行
    for (int i = 0; i < count; i++) {  // 每个像素独立
        pixels[i].color = ComputeColor(pixels[i].uv);  // 简单计算
    }
    // GPU: 同时处理所有像素 → 超快！
}

// 场景 2：不适合 GPU 的任务（复杂逻辑 + 依赖）
int Fibonacci(int n) {
    if (n <= 1) return n;  // 分支
    return Fibonacci(n-1) + Fibonacci(n-2);  // 递归 + 依赖前序结果
    // GPU: 无法并行，每个核心傻等 → 浪费！
}

// 场景 3：不适合 GPU 的任务（数据依赖）
void BubbleSort(int* array, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size - i - 1; j++) {
            if (array[j] > array[j+1]) {  // 比较
                Swap(array[j], array[j+1]);  // 依赖前序结果
            }
        }
    }
    // GPU: 每步依赖前一步，无法并行 → 慢！
}

总结：
GPU 快的条件：
✅ 大量数据（数百万）
✅ 独立计算（每个数据不依赖其他）
✅ 简单逻辑（少分支）
✅ 数学密集（大量乘加运算）

GPU 慢的情况：
❌ 少量数据（几百个）
❌ 串行依赖（前后关联）
❌ 复杂分支（大量 if-else）
❌ 递归调用
```

---

## 2. GPU 硬件架构详解

### 2.1 GPU 层次结构

```
┌─────────────────────────────────────────────────────────┐
│                    GPU 芯片                              │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌────────────────────────────────────────────────┐    │
│  │        GPC 0 (图形处理集群)                     │    │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐     │    │
│  │  │  TPC 0   │  │  TPC 1   │  │  TPC 2   │ ... │    │  ← TPC: 纹理处理集群
│  │  │ ┌──────┐ │  │ ┌──────┐ │  │ ┌──────┐ │     │    │
│  │  │ │ SM 0 │ │  │ │ SM 1 │ │  │ │ SM 2 │ │     │    │  ← SM: 流多处理器
│  │  │ └──────┘ │  │ └──────┘ │  │ └──────┘ │     │    │    (核心执行单元)
│  │  │ ┌──────┐ │  │ ┌──────┐ │  │ ┌──────┐ │     │    │
│  │  │ │ SM 1 │ │  │ │ SM 3 │ │  │ │ SM 5 │ │     │    │
│  │  │ └──────┘ │  │ └──────┘ │  │ └──────┘ │     │    │
│  │  └──────────┘  └──────────┘  └──────────┘     │    │
│  └────────────────────────────────────────────────┘    │
│                                                          │
│  ┌────────────────────────────────────────────────┐    │
│  │        GPC 1                                     │    │
│  │  (类似 GPC 0，多个 TPC 和 SM)                   │    │
│  └────────────────────────────────────────────────┘    │
│                                                          │
│  ... GPC 2, GPC 3, ... (RTX 4090 有 12 个 GPC)          │
│                                                          │
│  ┌────────────────────────────────────────────────┐    │
│  │        内存控制器                                │    │
│  │  ┌──────┬──────┬──────┬──────┬──────┬──────┐  │    │
│  │  │GDDR6X│GDDR6X│GDDR6X│GDDR6X│GDDR6X│GDDR6X│  │    │  ← 显存芯片
│  │  │ 2GB  │ 2GB  │ 2GB  │ 2GB  │ 2GB  │ 2GB  │  │    │    (共 12-24 GB)
│  │  └──────┴──────┴──────┴──────┴──────┴──────┘  │    │
│  └────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────┘

规模示例 (NVIDIA RTX 4090)：
- GPC: 12 个
- SM: 128 个
- CUDA 核心: 16,384 个 (每个 SM 有 128 个)
- 纹理单元: 512 个
- 显存: 24 GB GDDR6X
```

### 2.2 流多处理器 (SM) 详解

**SM 是 GPU 的基本执行单元**，所有并行计算都在 SM 中进行。

```
┌────────────────────────────────────────────────────────┐
│         SM (Streaming Multiprocessor)                  │
│         流多处理器                                      │
├────────────────────────────────────────────────────────┤
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   Warp Scheduler (线程束调度器)                  │  │  ← 控制单元
│  │   - 从线程池选择 32 个线程 (1 个 warp)           │  │
│  │   - 发射指令到执行单元                           │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   CUDA Cores (64-128 个)                         │  │  ← 算术单元
│  │                                                   │  │
│  │   ┌───┐┌───┐┌───┐┌───┐     ┌───┐┌───┐┌───┐     │  │
│  │   │FP32││FP32││FP32││FP32│...│FP32││FP32││FP32│  │  │  ← 32 位浮点
│  │   └───┘└───┘└───┘└───┘     └───┘└───┘└───┘     │  │
│  │   ┌───┐┌───┐┌───┐┌───┐     ┌───┐┌───┐┌───┐     │  │
│  │   │INT││INT││INT││INT│ ... │INT││INT││INT│     │  │  ← 整数
│  │   └───┘└───┘└───┘└───┘     └───┘└───┘└───┘     │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   Tensor Cores (第 4 代 - RTX 40 系列)           │  │  ← AI 加速
│  │   - 矩阵运算 (4x4)                               │  │
│  │   - FP16/INT8 快速计算                           │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   Texture Units (纹理单元)                       │  │  ← 纹理采样
│  │   - 4 个 (每个 SM)                               │  │
│  │   - 硬件过滤、Mipmap                             │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   Register File (寄存器文件)                     │  │  ← 超快存储
│  │   - 65,536 个 32 位寄存器                        │  │    (每个线程独立)
│  │   - 延迟: 1 周期                                 │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   Shared Memory (共享内存)                       │  │  ← 线程通信
│  │   - 128 KB (可配置)                              │  │
│  │   - 同一 Block 内线程共享                        │  │
│  │   - 延迟: ~30 周期                               │  │
│  └─────────────────────────────────────────────────┘  │
│                                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │   L1 Cache (一级缓存)                            │  │  ← 纹理/全局内存缓存
│  │   - 128 KB (与 Shared Memory 共享)              │  │
│  │   - 延迟: ~80 周期                               │  │
│  └─────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────┘

关键数字 (每个 SM)：
- 最大并发线程: 1536-2048 个
- 寄存器: 65,536 个 × 32 位 = 256 KB
- 共享内存 + L1: 128 KB
- 纹理单元: 4 个
```

### 2.3 Warp（线程束）：GPU 执行的基本单位

**关键概念**：GPU 不是以单个线程为单位执行，而是以 **Warp（32 个线程）** 为单位。

```
线程组织层次：

Grid (网格) - 整个 GPU 任务
  ↓
Block (线程块) - 分配到一个 SM
  ↓
Warp (线程束) - 32 个线程，同时执行相同指令 ← 关键！
  ↓
Thread (线程) - 单个线程

例子：渲染 1920x1080 图像

Grid:
- 总线程数: 1920 × 1080 = 2,073,600 个线程
- 每个线程处理 1 个像素

Blocks:
- Block 大小: 16 × 16 = 256 个线程
- Block 数量: 2,073,600 / 256 = 8,100 个 blocks

Warps:
- 每个 Block: 256 / 32 = 8 个 warps
- 总 Warp 数: 8,100 × 8 = 64,800 个 warps
```

#### Warp 执行模型（SIMT）

**SIMT = Single Instruction, Multiple Threads**（单指令多线程）

```
SIMT 执行示例：

着色器代码：
float4 main(float2 uv : TEXCOORD) : SV_Target {
    float y = yTexture.Sample(sampler, uv).r;      // 指令 1
    float u = uTexture.Sample(sampler, uv).r;      // 指令 2
    float v = vTexture.Sample(sampler, uv).r;      // 指令 3
    
    float r = y + 1.402 * v;                       // 指令 4
    float g = y - 0.344 * u - 0.714 * v;           // 指令 5
    float b = y + 1.772 * u;                       // 指令 6
    
    return float4(r, g, b, 1.0);                   // 指令 7
}

Warp 执行（32 个线程同时）：

周期 1: 所有 32 个线程执行 "指令 1" (采样 Y)
  线程 0: 采样像素 (0, 0) 的 Y 值
  线程 1: 采样像素 (1, 0) 的 Y 值
  ...
  线程 31: 采样像素 (31, 0) 的 Y 值

周期 2: 所有 32 个线程执行 "指令 2" (采样 U)
  [同时进行]

周期 3: 所有 32 个线程执行 "指令 3" (采样 V)
  [同时进行]

周期 4: 所有 32 个线程执行 "指令 4" (计算 R)
  [同时进行]

...

总耗时: 7 个周期 × 32 个线程 = 处理 32 个像素
吞吐量: 32 像素 / 7 周期 ≈ 4.6 像素/周期

对比 CPU:
- 1 个核心, 7 个周期 = 1 个像素
- 吞吐量: 1 像素 / 7 周期 ≈ 0.14 像素/周期
- GPU 快 32 倍！
```

#### Warp 发散（分支问题）

```cpp
// 问题代码：有分支
float4 main(float2 uv : TEXCOORD) : SV_Target {
    float y = yTexture.Sample(sampler, uv).r;
    
    if (uv.x < 0.5) {  // 🚨 分支！
        // 左半边：红色
        return float4(1, 0, 0, 1);
    } else {
        // 右半边：蓝色
        return float4(0, 0, 1, 1);
    }
}

Warp 执行问题：

假设 Warp 中的 32 个线程：
- 线程 0-15: uv.x < 0.5 (左半边)
- 线程 16-31: uv.x >= 0.5 (右半边)

执行过程：
周期 1: 评估分支条件 (所有 32 个线程)
  [并行]

周期 2-N: 执行 if 分支 (左半边)
  线程 0-15: ✅ 执行红色代码
  线程 16-31: ❌ 空闲等待 (浪费!)

周期 N+1-M: 执行 else 分支 (右半边)
  线程 0-15: ❌ 空闲等待 (浪费!)
  线程 16-31: ✅ 执行蓝色代码

结果：
- 实际执行时间 = if 时间 + else 时间
- 相当于串行执行两个分支
- 效率: 50% (一半线程总在等待)

┌─────────────────────────────────────┐
│  Warp 发散效率                      │
├─────────────────────────────────────┤
│  无分支: 100% 效率 ████████████████ │
│  2 分支: 50% 效率  ████████         │
│  4 分支: 25% 效率  ████             │
│  8 分支: 12.5%     ██               │
└─────────────────────────────────────┘
```

**如何避免 Warp 发散**：

```cpp
// ❌ 坏的做法：复杂分支
float4 BadShader(float2 uv) {
    if (uv.x < 0.25) {
        return ComputeComplexEffect1(uv);
    } else if (uv.x < 0.5) {
        return ComputeComplexEffect2(uv);
    } else if (uv.x < 0.75) {
        return ComputeComplexEffect3(uv);
    } else {
        return ComputeComplexEffect4(uv);
    }
}
// 效率: 25% (4 个分支)

// ✅ 好的做法 1：无分支计算
float4 GoodShader1(float2 uv) {
    // 使用数学公式代替分支
    float blend = step(0.5, uv.x);  // uv.x < 0.5 ? 0.0 : 1.0
    float4 leftColor = float4(1, 0, 0, 1);
    float4 rightColor = float4(0, 0, 1, 1);
    return lerp(leftColor, rightColor, blend);
}
// 效率: 100%

// ✅ 好的做法 2：提前分组
// 在 CPU 端将不同类型的像素分组
// 每个 Warp 处理相同类型的像素
// GPU 端无分支
```

### 2.4 GPU 时钟周期与延迟

```
GPU vs CPU 时钟：

CPU (Intel i9-13900K):
- 频率: 5.8 GHz
- 周期: 1 / 5.8e9 ≈ 0.17 ns
- 单指令延迟: 1-4 周期 = 0.17-0.7 ns

GPU (RTX 4090):
- 频率: 2.5 GHz
- 周期: 1 / 2.5e9 = 0.4 ns
- 单指令延迟: 10-400 周期 = 4-160 ns

为什么 GPU 单线程这么慢？
因为 GPU 不靠单线程速度，靠大量并行！

延迟隐藏 (Latency Hiding)：
┌────────────────────────────────────┐
│  指令延迟: 100 周期                │
├────────────────────────────────────┤
│  Warp 0: 开始执行 ────────→ 完成   │
│  Warp 1:   开始执行 ────────→ 完成 │
│  Warp 2:     开始执行 ────────→ 完成│
│  ...                               │
│  Warp 31:          开始执行 ──→ 完成│
└────────────────────────────────────┘
时间 →

技巧：
- 当 Warp 0 等待内存时（100 周期）
- SM 切换到 Warp 1 继续执行
- Warp 1 等待时，切换到 Warp 2
- ...
- 当 Warp 0 的数据到达时，切换回 Warp 0

结果：
- 每个周期都有 Warp 在执行
- 延迟被隐藏了
- 吞吐量 = 最大化！

这就是为什么 SM 需要支持 1536+ 个并发线程！
```

---

## 3. 并行计算原理

### 3.1 数据并行 vs 任务并行

#### 数据并行（GPU 擅长）

```
任务：对 1000 个数字执行相同操作

数据并行：
数据: [1, 2, 3, 4, ..., 1000]
操作: x → x * 2

执行:
线程 0: 1 → 2
线程 1: 2 → 4
线程 2: 3 → 6
...
线程 999: 1000 → 2000

[所有线程同时执行相同代码，处理不同数据]

代码:
// GPU kernel
__global__ void DoubleArray(float* data, int count) {
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    if (i < count) {
        data[i] = data[i] * 2.0;  // 所有线程执行相同操作
    }
}

特点:
✅ 完美并行（无依赖）
✅ 无需通信
✅ GPU 最擅长
```

#### 任务并行（CPU 擅长）

```
任务：执行不同的操作

任务并行：
线程 0: 读取文件
线程 1: 解码视频
线程 2: 渲染 UI
线程 3: 处理网络

[每个线程执行不同代码]

代码:
// CPU 多线程
std::thread t1(ReadFile);
std::thread t2(DecodeVideo);
std::thread t3(RenderUI);
std::thread t4(HandleNetwork);

t1.join(); t2.join(); t3.join(); t4.join();

特点:
✅ 灵活（不同任务）
✅ 适合复杂逻辑
❌ 需要同步和通信
❌ GPU 不擅长
```

### 3.2 渲染中的数据并行

```
场景：渲染 1920x1080 图像

每个像素独立计算（数据并行）：

┌───┬───┬───┬───┬───┐
│ P0│ P1│ P2│ P3│...│  每个像素是独立的数据
├───┼───┼───┼───┼───┤
│P1920│P1921│...  │  每个像素执行相同的着色器代码
├───┼───┼───┼───┼───┤
│...                │
└───────────────────┘

着色器代码 (每个像素执行):
float4 PixelShader(float2 uv) {
    // 1. 采样纹理
    float y = yTex.Sample(sampler, uv).r;
    float u = uTex.Sample(sampler, uv).r;
    float v = vTex.Sample(sampler, uv).r;
    
    // 2. YUV → RGB 转换
    float r = y + 1.402 * v;
    float g = y - 0.344 * u - 0.714 * v;
    float b = y + 1.772 * u;
    
    // 3. 返回颜色
    return float4(r, g, b, 1.0);
}

并行执行:
- 2,073,600 个线程
- 同时执行相同代码
- 每个线程处理不同像素 (uv)
- 无需通信，无依赖
- 完美并行！

时间复杂度:
CPU 串行: O(n) - 处理每个像素需要 n 个时间单位
GPU 并行: O(1) - 所有像素同时处理（理想情况）
实际: O(n/p) - p 是并行度
```

### 3.3 并行效率分析

#### Amdahl 定律（阿姆达尔定律）

```
公式：
加速比 = 1 / ((1 - P) + P/N)

其中：
- P: 可并行部分的比例
- N: 处理器数量（并行度）
- (1-P): 必须串行的部分

例子：渲染任务

场景 1: 99% 可并行
P = 0.99, N = 1000

加速比 = 1 / (0.01 + 0.99/1000)
       = 1 / 0.01099
       ≈ 91 倍

场景 2: 90% 可并行
P = 0.90, N = 1000

加速比 = 1 / (0.10 + 0.90/1000)
       = 1 / 0.1009
       ≈ 9.9 倍

场景 3: 50% 可并行
P = 0.50, N = 1000

加速比 = 1 / (0.50 + 0.50/1000)
       = 1 / 0.5005
       ≈ 2 倍

结论：
即使有 1000 个核心，如果只有 50% 可并行，
最多只能快 2 倍！

这就是为什么：
- 渲染快（99% 并行）→ GPU 快 50-100 倍
- 复杂 AI（70% 并行）→ GPU 快 5-10 倍
- 操作系统（20% 并行）→ GPU 可能更慢
```

#### 并行开销

```
并行不是免费的，有开销：

1. 启动开销
   - CPU 需要准备数据、启动 GPU kernel
   - 通常: 10-100 微秒
   
2. 数据传输开销
   - CPU → GPU: 通过 PCIe 总线
   - 带宽: ~16 GB/s
   - 延迟: 几微秒
   
3. 同步开销
   - 等待 GPU 完成
   - CPU 和 GPU 同步点

例子：小任务不适合 GPU

任务：计算 100 个数的平方

CPU:
- 计算时间: 100 × 10 ns = 1 微秒
- 总时间: 1 微秒

GPU:
- 启动开销: 50 微秒
- 数据传输 (100 × 4 字节 × 2): 0.05 微秒
- 计算时间: 0.01 微秒
- 总时间: 50.06 微秒

结果: CPU 快 50 倍！

何时使用 GPU？
经验法则：
- 数据量 > 1 MB: GPU 可能更快
- 计算量 > 10 ms: GPU 更快
- 数据量小 + 计算简单: CPU 更快
```

---

## 4. GPU 内存系统

### 4.1 内存层次结构

```
┌─────────────────────────────────────────────────────────┐
│  内存层次 (从快到慢)                                     │
├──────────────┬──────────┬──────────┬────────────────────┤
│  名称        │  大小    │  延迟    │  带宽               │
├──────────────┼──────────┼──────────┼────────────────────┤
│  寄存器      │  256 KB  │  1 周期  │  ~20 TB/s          │
│  (Register)  │ (每 SM)  │ (~0.4ns) │  (理论最大)        │
├──────────────┼──────────┼──────────┼────────────────────┤
│  共享内存    │  128 KB  │ 30 周期  │  ~10 TB/s          │
│  (Shared)    │ (每 SM)  │ (~12ns)  │  (每 SM)           │
├──────────────┼──────────┼──────────┼────────────────────┤
│  L1 缓存     │  128 KB  │ 80 周期  │  ~5 TB/s           │
│  (L1 Cache)  │ (每 SM)  │ (~32ns)  │  (每 SM)           │
├──────────────┼──────────┼──────────┼────────────────────┤
│  L2 缓存     │  72 MB   │ 200 周期 │  ~3 TB/s           │
│  (L2 Cache)  │ (全局)   │ (~80ns)  │  (全局共享)        │
├──────────────┼──────────┼──────────┼────────────────────┤
│  显存        │  24 GB   │ 400 周期 │  ~1 TB/s           │
│  (VRAM)      │ (全局)   │ (~160ns) │  (理论峰值)        │
├──────────────┼──────────┼──────────┼────────────────────┤
│  系统内存    │  32 GB   │ 1000周期 │  ~16 GB/s          │
│  (RAM)       │ (CPU)    │ (~400ns) │  (PCIe 4.0 x16)    │
└──────────────┴──────────┴──────────┴────────────────────┘

延迟倍数：
寄存器: 1x
共享内存: 30x
L1 缓存: 80x
L2 缓存: 200x
显存: 400x
系统内存: 1000x
```

### 4.2 寄存器（最快）

```cpp
// 寄存器：线程私有，最快

__global__ void ExampleKernel() {
    // 这些变量存储在寄存器中
    float a = 1.0;     // 寄存器
    float b = 2.0;     // 寄存器
    float c = a + b;   // 读写寄存器，1 周期
    
    // 访问速度：
    // c = a + b; 
    // 需要：读 a (1 周期) + 读 b (1 周期) + 加法 (1 周期) = 3 周期
}

限制：
- 每个 SM 只有 65,536 个寄存器
- 如果一个线程用太多寄存器，会减少并发线程数

例子：
- 每线程 32 个寄存器 → 最多 2048 个并发线程 ✅
- 每线程 64 个寄存器 → 最多 1024 个并发线程 ⚠️
- 每线程 128 个寄存器 → 最多 512 个并发线程 ❌ (太少)

优化建议：
- 减少局部变量
- 重用变量
- 编译器优化: -maxrregcount=32
```

### 4.3 共享内存（线程通信）

```cpp
// 共享内存：Block 内线程共享

__global__ void MatrixMultiply(float* A, float* B, float* C) {
    // 声明共享内存 (整个 Block 共享)
    __shared__ float As[16][16];  // 16×16 = 256 个浮点数
    __shared__ float Bs[16][16];
    
    // 每个线程从全局内存加载数据到共享内存
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    As[ty][tx] = A[...];  // 从显存加载 (慢)
    Bs[ty][tx] = B[...];
    
    // 同步：确保所有线程都加载完成
    __syncthreads();
    
    // 现在可以从共享内存快速读取
    float sum = 0;
    for (int k = 0; k < 16; k++) {
        sum += As[ty][k] * Bs[k][tx];  // 从共享内存读取 (快！)
    }
    
    C[...] = sum;
}

性能提升：
不使用共享内存:
- 每次乘法从显存读取: 400 周期
- 16 次乘法: 16 × 400 = 6400 周期

使用共享内存:
- 一次性加载到共享内存: 400 周期
- 16 次乘法从共享内存读取: 16 × 30 = 480 周期
- 总计: 400 + 480 = 880 周期
- 提速: 6400 / 880 ≈ 7.3 倍！
```

### 4.4 显存（最大但最慢）

```cpp
// 显存：所有线程可访问，但延迟高

// 纹理存储在显存中
ID3D11Texture2D* texture;  // 在显存中

// 读取显存（在着色器中）
float4 color = texture.Sample(sampler, uv);  // 400 周期延迟

// 但 GPU 通过并行隐藏延迟：
// - Warp 0 开始采样 (等待 400 周期)
// - 切换到 Warp 1 继续执行
// - 当 Warp 0 的数据到达时，继续执行
// - 延迟被隐藏了！
```

---

## 5. 为什么渲染适合 GPU

### 5.1 渲染的特点

```
✅ 大量数据
- 1080p: 2,073,600 像素
- 4K: 8,294,400 像素
- 8K: 33,177,600 像素

✅ 独立计算
- 每个像素独立
- 像素 A 的颜色不影响像素 B

✅ 简单重复
- 每个像素执行相同的着色器代码
- YUV → RGB 转换（简单数学）

✅ 数学密集
- 大量乘加运算
- 插值计算
- 纹理采样

✅ 少分支
- 通常无 if-else
- 或者分支预测性高

✅ 高内存带宽需求
- 读取纹理数据
- 写入帧缓冲

完美匹配 GPU 特性！
```

### 5.2 渲染流水线并行分析

```
视频帧渲染步骤：

1. 顶点处理
   - 输入：4 个顶点（全屏四边形）
   - 并行度：4（太小，CPU 也行）
   
2. 光栅化
   - 输入：2 个三角形
   - 输出：2,073,600 个像素
   - 并行度：很高 ✅
   
3. 像素着色（关键！）
   - 输入：2,073,600 个像素
   - 每像素：
     * 采样 3 个纹理（Y, U, V）
     * 10 次浮点运算
     * 1 次写入
   - 总计算：20,736,000 次操作
   - 并行度：超高 ✅✅✅
   
4. 输出合并
   - 写入帧缓冲
   - 并行度：高 ✅

可并行比例：
- 串行部分：设置状态 < 1%
- 并行部分：像素处理 > 99%

根据 Amdahl 定律：
加速比 ≈ 1 / (0.01 + 0.99/N)
N = 1000 时，加速比 ≈ 91 倍

实际测量：
- CPU: 30 ms/帧 (33 fps)
- GPU: 1 ms/帧 (1000 fps)
- 实际加速：30 倍

为什么不是 91 倍？
- 内存带宽限制
- 启动和同步开销
- 缓存未命中
- 但仍然非常快！
```

---

## 📚 总结

### 核心概念回顾

1. **CPU vs GPU 设计哲学**
   - CPU：少而强的核心，擅长复杂逻辑
   - GPU：多而简单的核心，擅长数据并行

2. **GPU 硬件架构**
   - 层次：GPC → TPC → SM → CUDA 核心
   - SM 是基本执行单元，包含数百个核心
   - Warp（32 个线程）是执行的最小单位

3. **SIMT 执行模型**
   - 所有线程执行相同指令
   - 分支导致 Warp 发散（效率下降）
   - 通过大量并发隐藏延迟

4. **GPU 内存系统**
   - 5 级层次：寄存器 → 共享内存 → L1 → L2 → 显存
   - 延迟差异：400 倍
   - 带宽差异：20 倍

5. **为什么渲染快**
   - 99% 可并行
   - 数据独立
   - 数学密集
   - 完美匹配 GPU 特性

### 关键数字

```
性能对比：
- 计算能力：GPU 是 CPU 的 40 倍
- 内存带宽：GPU 是 CPU 的 20 倍
- 渲染速度：GPU 比 CPU 快 15-30 倍

GPU 规模：
- CUDA 核心：16,384 个
- SM：128 个
- 并发线程：200,000+ 个
- 显存带宽：1 TB/s
```

---

## 🚀 下一步

在 **Part 2B** 中，我们将学习：
- **完整的渲染管线**（顶点 → 像素 → 输出）
- **顶点着色器和像素着色器**
- **光栅化原理**
- **渲染状态管理**
- **实际着色器代码示例**

准备好继续了吗？告诉我何时开始 Part 2B！ 🎯
